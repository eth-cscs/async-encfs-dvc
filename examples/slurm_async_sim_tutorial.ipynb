{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1e3f1f-753d-462b-88bd-b31d918896c1",
   "metadata": {},
   "source": [
    "# Tutorial: Running DVC stages asynchronously with SLURM using EncFS and Sarus\n",
    "\n",
    "This guide shows how to configure app-policies to run an application in a SLURM cluster within [Sarus](https://products.cscs.ch/sarus/) containers while operating on encrypted data managed through EncFS. We focus on an asynchronous execution model as familiar from `sbatch`. That is when reproducing a stage, DVC will only be blocking during job submission to the SLURM queue, but not wait until the resources have been allocated, the job executed and the output data is available.\n",
    "\n",
    "Familiarity with the basic constructs such as how to construct a DVC repository with infrastructure-as-code techniques in the [tutorial for an ML repository](ml_tutorial.ipynb) and running [DVC stages on EncFS and Docker](encfs_sim_tutorial.ipynb) is assumed. We will use the same iterative simulation workflow as in the EncFS-tutorial as an example application. The key steps we will cover include:\n",
    "\n",
    " * Configure app-policies to run DVC stages on a SLURM cluster\n",
    " * Submit DVC stages that are asynchronously completed, committed and pushed to the SLURM queue\n",
    " * Monitor and control SLURM jobs corresponding to a DVC stage\n",
    "\n",
    "The integration of these features with support for containers and EncFS is seamless, i.e. a previously developed application using them can be configured without any invasive code changes to run on a SLURM cluster.\n",
    "\n",
    "As a prerequisite to this tutorial, you should have `encfs` installed as described in the [instructions](../async_encfs_dvc/encfs_int/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276f6c3-08cf-409b-b1e3-6e767e0e82db",
   "metadata": {},
   "source": [
    "## Initializing the DVC repository\n",
    "We first import the depencies for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8552e-1d3e-4c31-b410-627b1de89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac94128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG  # test_slurm_async_sim_tutorial: skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65be096-eb8e-4d18-a2e2-858ea136b47b",
   "metadata": {},
   "source": [
    "Create a new directory `data/v2` for the DVC root and change to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd315e-6ab4-4e75-8a85-4056b09563ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data/v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823a232-0796-405f-af6d-ecda49add286",
   "metadata": {},
   "source": [
    "Initialize an `encfs` DVC repository using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f12d8-3b44-40f4-8bd2-ffb2dd84fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc_init_repo . encfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d1a24",
   "metadata": {},
   "source": [
    "As next step, EncFS needs to be configured, which can be achieved by running\n",
    "\n",
    "```shell\n",
    "${ENCFS_INSTALL_DIR}/bin/encfs -o allow_root,max_write=1048576,big_writes -f encrypt decrypt\n",
    "```\n",
    "as described in the [EncFS initialization instructions](../async_encfs_dvc/encfs_int/README.md).\n",
    "\n",
    "Here, only for the purpose of this tutorial, we use a pre-established configuration with a simple password. It is important that this is only for demonstration purposes - in practice always generate a **random** key and store it in a **safe location**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo 1234 > encfs_tutorial.key\n",
    "cp $(git rev-parse --show-toplevel)/examples/.encfs6.xml.tutorial encrypt/ && mv encrypt/.encfs6.xml.tutorial encrypt/.encfs6.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06397c8",
   "metadata": {},
   "source": [
    "At runtime, EncFS will read the password from a file. The location of that file is passed in an environment variable that has to be set when `dvc repro` is run on a stage or `encfs_launch` is used to e.g. inspect the encrypted data interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ENCFS_PW_FILE'] = os.path.realpath('encfs_tutorial.key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e1e70-9135-4c7c-954f-af0c5b850341",
   "metadata": {},
   "source": [
    "The DVC repo has been initialized with repo and stage policies available under `.dvc_policies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25ced6-1813-4891-895a-8963bb46a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree .dvc_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352c862",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we will extract the paths of the encrypted directory and the mount target of EncFS into environment variables. This is not a necessary step to run DVC stages with EncFS, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from async_encfs_dvc.encfs_int.mount_config import load_mount_config\n",
    "\n",
    "mount_config = [os.popen(f\"echo {d}\").read().strip() for d in  # evaluating shell exprs in paths\n",
    "                load_mount_config('.dvc_policies/repo/dvc_root.yaml')]\n",
    "\n",
    "os.environ['ENCFS_ENCRYPT_DIR'] = mount_config[0]  # encrypt (same on all hosts)\n",
    "os.environ['ENCFS_DECRYPT_DIR'] = mount_config[1]  # host-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1eae6-bdeb-491c-a3dc-1d8ae82b6e6c",
   "metadata": {},
   "source": [
    "## Establishing the input dataset\n",
    "Our pipeline will be based on a dataset labeled `sim_dataset_v1` and thereof a specific subset `ex2` (label chosen arbitrarily). First, we create a DVC stage to track the encrypted input data. As `dvc add` on longer supports the `--file` option to locate the `.dvc` file in a different folder, we use a workaround with a frozen no-op stage analogous to the manual preprocessing step for the training dataset in the ML tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd5611-cf31-4e6e-bab2-5307237c0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc_create_stage --app-yaml ../../in/dvc_app.yaml --stage add --dataset-name sim_dataset_v1 --subset-name ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0a293-c0f9-4390-959c-8a681d4d3baa",
   "metadata": {},
   "source": [
    "We now populate this dataset repository with input data that we generate randomly here, even though it could be downloaded from a remote source. As this will be encrypted data, we have to run `dd` that generates this data in an environment managed by EncFS, which is achieved by wrapping the command with `encfs_mount_and_run`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa28d0-9aa6-4847-aa0d-53f40385ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "encfs_mount_and_run encrypt ${ENCFS_DECRYPT_DIR} ${ENCFS_DECRYPT_DIR}/in/sim_dataset_v1/original/ex2/output/encfs_out.log \\\n",
    "    dd if=/dev/urandom of=${ENCFS_DECRYPT_DIR}/in/sim_dataset_v1/original/ex2/output/sim_in.dat bs=4k iflag=fullblock,count_bytes count=$((10**7)) \\\n",
    "    > config/in/sim_dataset_v1/original/ex2/output/stage_out.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca043ea7-dfb2-4d87-9873-9160eace6252",
   "metadata": {},
   "source": [
    "We can inspect the logs of the outer command, that is of `encfs_mount_and_run`, which we capture in the file `stage_out.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34135337-9d40-4b78-a192-44eb71cdbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat config/in/sim_dataset_v1/original/ex2/output/stage_out.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47079eb8-0edb-4f9f-95d2-545a41651a02",
   "metadata": {},
   "source": [
    "These logs are unencrypted and do not leak any details on the application inside the EncFS-environment. The application logs are captured separately in the encrypted file `encfs_out.log`. To inspect it, we need to mount a decrypted view of `encrypt`. In a typical session, a user would launch EncFS in the foreground on a another terminal using\n",
    "```shell\n",
    "encfs_launch\n",
    "```\n",
    "and then access the decrypted data here. However, *exclusively* for the purpose of this notebook, we will copy the decrypted data to an unencrypted location to inspect it outside of EncFS. We emphasize that this is only for demonstration purposes and confidential data should *never* be handled in that manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24fd1f-da42-4658-85a7-de18cb4e2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "encfs_mount_and_run encrypt ${ENCFS_DECRYPT_DIR} /dev/null cp ${ENCFS_DECRYPT_DIR}/in/sim_dataset_v1/original/ex2/output/encfs_out.log encfs_out.log >/dev/null\n",
    "cat encfs_out.log\n",
    "rm encfs_out.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af551d9-8c27-42d3-9c0c-ab1f7fac737a",
   "metadata": {},
   "source": [
    "To avoid logging the output of an EncFS-managed application to a file, you can supply `/dev/null` as a third parameter to `encfs_mount_and_run` as done here.\n",
    "\n",
    "Finally, we commit the newly added data to DVC as described in the manual preprocessing step for the training dataset in the ML tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564ba49-89bf-4d0c-b2e0-1ec824df756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc commit --force config/in/sim_dataset_v1/original/ex2/dvc.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a331d-ac6d-464a-805f-9ce38de6222e",
   "metadata": {},
   "source": [
    "The resulting file hierarchy looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102870a6-3156-4b36-b72e-b368dca773e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree encrypt/in config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c3900-edd2-4cb0-8fb1-1fcba605624a",
   "metadata": {},
   "source": [
    "Before moving to the definition of preprocessing stages, we define execution labels based on timestamps for the subsequent DVC stages. In a real-world application, the timestamps would usually be generated on the fly when creating the DVC stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5a9cc-eee6-4c68-b9fe-46e195a0619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ETL_RUN_LABEL=ex2-20230714-083624\n",
    "%env SIM_0_RUN_LABEL=ex2-20230714-083812-0\n",
    "%env SIM_1_RUN_LABEL=ex2-20230714-083812-1\n",
    "%env SIM_2_RUN_LABEL=ex2-20230714-083812-2\n",
    "%env SIM_3_RUN_LABEL=ex2-20230714-083812-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9e318-748a-4d20-a90b-f352e23dcca7",
   "metadata": {},
   "source": [
    "## Constructing the preprocessing stage\n",
    "The next step involves setting up the preprocessing stage. For illustration purposes, we will use the same preprocessing application as in the ML repository tutorial. To simplify things, we do not run this step over SLURM yet, though this could be changed without difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76eca4-4dd0-4b68-aeda-ed390736f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc_create_stage --app-yaml ../../app_prep/dvc_app.yaml --stage sim \\\n",
    "    --run-label ${ETL_RUN_LABEL} --input-etl ex2 --input-etl-file output/sim_in.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c44d08-1860-44c3-9642-302a92b032b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc repro config/in/sim_dataset_v1/app_prep_v1/auto/${ETL_RUN_LABEL}/dvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2b7db-30e3-4e11-8f86-7e83c234a56b",
   "metadata": {},
   "source": [
    "This will execute the stage synchronously within the DVC-launched process. Execution can also be deferred to the simulation stages, where it will be triggered as a dependency. However, it is not recommended to launch mixed graphs of synchronous and asynchronous (SLURM) stages. In particular, asynchronous stages must not be launched as ancestors of synchronous DVC stages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22280d7e-cae3-4df3-8674-2201925fceeb",
   "metadata": {},
   "source": [
    "## Creating the simulation stages for SLURM\n",
    "Lastly, we will establish a rudimentary structure for an iterative simulation workflow that utilizes the preprocessed data. A usual simulation comes with its own tunable parameters. For this purpose, one can use a file such as for hyperparameters in the machine learning tutorial. However, for brevity here we skip this step and assume that all parameters are passed through the command line.\n",
    "\n",
    "To enable stages to be run with SLURM, we add `slurm_opts` to the application policy under the corresponding stages (here `base_simulation` and `simulation`). Upon reproducing a DVC SLURM stage, the stage dependency graph will be submitted to the SLURM queue as a corresponding job graph. Multiple jobs are created per DVC stage, specifically a stage job for the actual application, upon its succes a DVC commit job and optionally a DVC push job and upon of stage job failure a cleanup job. There is a one-to-one correspondence between dependencies of DVC stages and those of SLURM stage jobs. In the application policy, we typically specify the options for stage jobs under `stage` and those for DVC operations like commit and push under `dvc`. For options that apply to all SLURM jobs we can use `all`.\n",
    "\n",
    "We are now ready to set up the simulation stages. Where necessary, we can obtain completion suggestions with `--show-opts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23763a0-4602-4932-895e-0967304ecb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc_create_stage --app-yaml ../../app_sim/dvc_app_slurm.yaml --stage base_simulation \\\n",
    "  --run-label ${SIM_0_RUN_LABEL} \\\n",
    "  --input-simulation ${ETL_RUN_LABEL} \\\n",
    "  --simulation-output-file-num-per-rank 3 \\\n",
    "  --simulation-output-file-size $((10**6 * 2**0 / 3))\n",
    "\n",
    "dvc_create_stage --app-yaml ../../app_sim/dvc_app_slurm.yaml --stage simulation \\\n",
    "  --run-label ${SIM_1_RUN_LABEL} \\\n",
    "  --input-simulation ${SIM_0_RUN_LABEL} \\\n",
    "  --simulation-output-file-num-per-rank 3 \\\n",
    "  --simulation-output-file-size $((10**6 * 2**1 / 3))\n",
    "\n",
    "dvc_create_stage --app-yaml ../../app_sim/dvc_app_slurm.yaml --stage simulation \\\n",
    "  --run-label ${SIM_2_RUN_LABEL} \\\n",
    "  --input-simulation ${SIM_1_RUN_LABEL} \\\n",
    "  --simulation-output-file-num-per-rank 3 \\\n",
    "  --simulation-output-file-size $((10**6 * 2**2 / 3))\n",
    "\n",
    "dvc_create_stage --app-yaml ../../app_sim/dvc_app_slurm.yaml --stage simulation \\\n",
    "  --run-label ${SIM_3_RUN_LABEL} \\\n",
    "  --input-simulation ${SIM_2_RUN_LABEL} \\\n",
    "  --simulation-output-file-num-per-rank 3 \\\n",
    "  --simulation-output-file-size $((10**6 * 2**3 / 3))\n",
    "\n",
    "tree encrypt config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b50a3d-a9e5-43a9-aff9-dcd44fa351a4",
   "metadata": {},
   "source": [
    "## Running and monitoring the pipeline with SLURM\n",
    "The stages can be inspected with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141181e-6424-4c28-86c0-6c729fa25dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc dag --dot config/app_sim_v1/sim_dataset_v1/simulation/${SIM_3_RUN_LABEL}/dvc.yaml | tee config/app_sim_v1/sim_dataset_v1/simulation/${SIM_3_RUN_LABEL}/dvc_dag.dot\n",
    "if [[ $(command -v dot) ]]; then\n",
    "    dot -Tsvg config/app_sim_v1/sim_dataset_v1/simulation/${SIM_3_RUN_LABEL}/dvc_dag.dot > config/app_sim_v1/sim_dataset_v1/simulation/${SIM_3_RUN_LABEL}/dvc_dag.svg\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cc186-b78b-4df0-a900-713b4e4168b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_dag_img = 'config/app_sim_v1/sim_dataset_v1/simulation/' + os.environ['SIM_3_RUN_LABEL'] + '/dvc_dag.svg'\n",
    "if os.path.exists(dvc_dag_img):\n",
    "    display(SVG(filename=dvc_dag_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093f914-a50b-4ee0-b94e-579cf8ec5c82",
   "metadata": {},
   "source": [
    "We can now submit the pipeline of asynchronous SLURM stages. Note that we use `dvc repro --no-commit` to submit asynchronous stages. This is in order not to add any output during SLURM job submission time to the DVC cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec8894-90c8-432a-876c-443a77333032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ ! -x \"$(command -v sarus)\" ]]; then  # slurm_enqueue checks availability of sarus\n",
    "    module load sarus\n",
    "fi\n",
    "dvc repro --no-commit config/app_sim_v1/sim_dataset_v1/simulation/${SIM_3_RUN_LABEL}/dvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38074287",
   "metadata": {},
   "source": [
    "The submitted SLURM stages can be monitored with a status file and log files in the `dvc.yaml` directory and the tool `dvc_scontrol`. The latter makes functionality of SLURM's `scontrol` available for asynchronous DVC stages and, thus, also allows to control DVC SLURM job groups. In particular, we can list the submitted jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc_scontrol show all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36126280",
   "metadata": {},
   "source": [
    "As the stage and commit jobs are put on hold in order to enable the submission of more DVC SLURM stages by the user, we have to release them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc_scontrol release stage,commit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f72914",
   "metadata": {},
   "source": [
    "This will enable resource allocation and execution of the application stages. Upon successfull execution, the stage outputs will also be committed. We can now log out from the SLURM cluster and return later to inspect results.\n",
    "\n",
    "For the purpose of this tutorial, however, we want to keep monitoring the pipeline on SLURM and wait for the completion of the last job, which is a DVC commit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# ID of last commit job\n",
    "commit_jobid=$(cat config/app_sim_v1/sim_dataset_v1/simulation/${SIM_3_RUN_LABEL}/app_sim_v1_sim_dataset_v1_simulation_${SIM_3_RUN_LABEL}.dvc_commit_jobid)\n",
    "../../slurm_wait_for_job.sh ${commit_jobid}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bb5af",
   "metadata": {},
   "source": [
    "Once the pipeline has completed, as described above, results can be inspected by running EncFS with `encfs_launch` from the DVC root directory in another terminal and then accessing the data through the decrypted directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456a0fe-49f4-42fb-a3de-61847c305369",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree encrypt config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6271c-3780-44c7-a73f-a72d22d5e5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
