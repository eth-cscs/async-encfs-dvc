{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1e3f1f-753d-462b-88bd-b31d918896c1",
   "metadata": {},
   "source": [
    "# Tutorial: Building a DVC repo for an ML workflow\n",
    "\n",
    "This guide demonstrates the construction of a DVC repository that is tailored to a machine learning pipeline with infrastructure-as-code principles. The example pipeline will perform dataset preprocessing and machine learning training/inference stages. The key concepts we will explore include:\n",
    "\n",
    " * Repository-wide configuration, managed via repo-policies\n",
    " * Stage-wise configuration, controlled through stage-policies\n",
    " * The creation of an app-policy that derives its parameters from the previous two, leading to a concrete instantiation of the corresponding DVC stages\n",
    "\n",
    "It is important to note that stage and app policies cover multiple logically connected DVC stages, not just a single stage.  For example, a machine learning app policy usually encapsulates both a training and inference stage of an ML model.\n",
    "\n",
    "In the context of an ML workflow, we'll cover how to\n",
    " * Handle manual and automated preprocessing steps in DVC\n",
    " * Set up an ML stage using these techniques\n",
    " * Execute the stages\n",
    "\n",
    "To streamline this tutorial, we will not be using `EncFS`, containers and SLURM. The focus will primarily be on the stage and app policies as well as on the file hierarchy. All additional features can conveniently be activated later via modifications to the app policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276f6c3-08cf-409b-b1e3-6e767e0e82db",
   "metadata": {},
   "source": [
    "## Initializing the DVC repository\n",
    "We first import the depencies for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8552e-1d3e-4c31-b410-627b1de89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac94128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG  # test_ml_tutorial: skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65be096-eb8e-4d18-a2e2-858ea136b47b",
   "metadata": {},
   "source": [
    "Create a new directory `data/v0` for the DVC root and change to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd315e-6ab4-4e75-8a85-4056b09563ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data/v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823a232-0796-405f-af6d-ecda49add286",
   "metadata": {},
   "source": [
    "Initialize a `plain` DVC repository using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f12d8-3b44-40f4-8bd2-ffb2dd84fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc_init_repo . plain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e1e70-9135-4c7c-954f-af0c5b850341",
   "metadata": {},
   "source": [
    "The DVC repo has been initialized with repo and stage policies available under `.dvc_policies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25ced6-1813-4891-895a-8963bb46a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree .dvc_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1eae6-bdeb-491c-a3dc-1d8ae82b6e6c",
   "metadata": {},
   "source": [
    "## Establishing the input dataset\n",
    "Our pipeline will be based on a dataset labeled `ml_dataset` that we assume to be split into training, test, and inference. Each of these has specific subsets and we will utilize a subset labeled `ex1` for all of them (although this could be chosen differently for each of them). We populate the repository with this input data and track it with DVC by executing the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa28d0-9aa6-4847-aa0d-53f40385ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p in/ml_dataset_v1/{training,test,inference}/original/ex1\n",
    "touch in/ml_dataset_v1/{training,test,inference}/original/ex1/in.dat\n",
    "for d in in/ml_dataset_v1/{training,test,inference}/original; do\n",
    "    cd $d && dvc add ex1 && cd -\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a331d-ac6d-464a-805f-9ce38de6222e",
   "metadata": {},
   "source": [
    "The resulting file hierarchy looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102870a6-3156-4b36-b72e-b368dca773e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c3900-edd2-4cb0-8fb1-1fcba605624a",
   "metadata": {},
   "source": [
    "Before moving to the definition of preprocessing stages, we define execution labels based on timestamps for the subsequent DVC stages. In a real-world application, the timestamps would usually be generated on the fly when creating the DVC stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5a9cc-eee6-4c68-b9fe-46e195a0619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ETL_TRAIN_RUN_LABEL=ex1-20230713-083624\n",
    "%env ETL_INF_RUN_LABEL=ex1-20230713-083812\n",
    "%env ETL_TEST_RUN_LABEL=ex1-20230713-083951\n",
    "%env ML_TRAIN_RUN_LABEL=ex1-20230713-090119\n",
    "%env ML_INF_RUN_LABEL=ex1-20230713-121007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9e318-748a-4d20-a90b-f352e23dcca7",
   "metadata": {},
   "source": [
    "## Constructing the preprocessing stage\n",
    "The next step involves setting up the preprocessing stages. For the purpose of the demonstration, we will assume that this consists of a simple copy operation for each of the training, test and inference data. In a real-world scenario, this can be replaced by any other operation as required.\n",
    "\n",
    "In particular, a preprocessing stage may also involve manual user interaction (e.g. in a GUI) that cannot be reproduced from the command line and, hence, is executed outside of DVC. We will cover this case for the training data, whereas we use DVC-reproducible commands for the preprocessing of the other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52279632-7466-4cf5-98e0-846fded33586",
   "metadata": {},
   "source": [
    "### Manual preprocessing\n",
    "To implement a manual preprocessing step for the `training` data, we create a DVC stage with a no-op command that is `frozen` in order not to be reproduced by `dvc repro` as configured in the app policy `dvc_app.yaml`. Nevertheless, the data dependencies of this stage are tracked in DVC via an ETL stage policy `dvc_etl.yaml`.\n",
    "\n",
    "To create this stage, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea71be-a51e-4539-b91b-9ee85a4e7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc_create_stage --app-yaml ../../app_prep/dvc_app.yaml --stage manual_train \\\n",
    "    --run-label ${ETL_TRAIN_RUN_LABEL} --input-etl ex1 --input-etl-file in.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32464eca-ecbc-46fe-9d31-df02a2ef0a32",
   "metadata": {},
   "source": [
    "To obtain the output data, the manual operation can now be performed. For this purpose, we inspect the newly created `dvc.yaml`. To perform a copy operation, we move the data in `deps` to the `output` directory in `outs`. As described above, in a real application, this step would typically be performed in a GUI app with some user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd746b3b-b193-4d07-b501-82a90299e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd in/ml_dataset_v1/training/app_prep_v1/manual/${ETL_TRAIN_RUN_LABEL}\n",
    "cat dvc.yaml\n",
    "cp ../../../original/ex1/* output/\n",
    "cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55710f11-05dd-432c-a1f2-170304450a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree in/ml_dataset_v1/training/app_prep_v1/manual/${ETL_TRAIN_RUN_LABEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07120c1-3a00-404a-8117-d879cef766b5",
   "metadata": {},
   "source": [
    "Once the manual operation has completed and the output directory is populated with data, we can commit it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033206e4-d8f0-4c01-aaec-9bb754ee0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc commit --force in/ml_dataset_v1/training/app_prep_v1/manual/${ETL_TRAIN_RUN_LABEL}/dvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bdda6-cc0c-4ae2-965f-57912d1d3a02",
   "metadata": {},
   "source": [
    "### Automated preprocessing\n",
    "In contrast, for the test and inference datasets, we will assume that the preprocessing is fully automated. These stages can be created and executed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7738a05a-dba6-43ec-89ce-51275263d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# test-data\n",
    "dvc_create_stage --app-yaml ../../app_prep/dvc_app.yaml --stage auto_test \\\n",
    "    --run-label ${ETL_TEST_RUN_LABEL} --input-etl ex1 --input-etl-file in.dat\n",
    "dvc repro in/ml_dataset_v1/test/app_prep_v1/auto/${ETL_TEST_RUN_LABEL}/dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76eca4-4dd0-4b68-aeda-ed390736f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# inference-data\n",
    "dvc_create_stage --app-yaml ../../app_prep/dvc_app.yaml --stage auto_inf \\\n",
    "    --run-label ${ETL_INF_RUN_LABEL} --input-etl ex1 --input-etl-file in.dat\n",
    "dvc repro in/ml_dataset_v1/inference/app_prep_v1/auto/${ETL_INF_RUN_LABEL}/dvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2b7db-30e3-4e11-8f86-7e83c234a56b",
   "metadata": {},
   "source": [
    "Execution can also be deferred to the ML stages, where it will be triggered as a dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24d37d-9dd0-4d70-8bc9-6517043afef6",
   "metadata": {},
   "source": [
    "## Creating the Machine Learning stage\n",
    "Lastly, we will establish a rudimentary structure for a machine learning application that utilizes the preprocessed data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b4837-26cc-42f2-b380-93622d07a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p app_ml/ml_dataset_v1/model_name_v2/{training,inference,config/ex1-config}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22280d7e-cae3-4df3-8674-2201925fceeb",
   "metadata": {},
   "source": [
    "We encapsulate hyperparameters and model architecture specifications that are fixed during training in a file `hp.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedc3fa-3321-4a09-8f79-f44f55f6ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch app_ml/ml_dataset_v1/model_name_v2/config/ex1-config/hp.yaml\n",
    "cd app_ml/ml_dataset_v1/model_name_v2/config && dvc add ex1-config && cd -\n",
    "tree app_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da4a45-331a-4a9c-8efa-33758d294102",
   "metadata": {},
   "source": [
    "Following this, we can set up the machine learning training and inference stages. Where necessary, we can obtain completion suggestions with `--show-opts`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23763a0-4602-4932-895e-0967304ecb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc_create_stage --app-yaml ../../app_ml/dvc_app.yaml --stage training \\\n",
    "    --run-label ${ML_TRAIN_RUN_LABEL} \\\n",
    "    --input-config ex1-config --input-config-file hp.yaml --input-training ${ETL_TRAIN_RUN_LABEL} --input-test ${ETL_TEST_RUN_LABEL}\n",
    "dvc_create_stage --app-yaml ../../app_ml/dvc_app.yaml --stage inference \\\n",
    "    --run-label ${ML_INF_RUN_LABEL} \\\n",
    "    --input-config ex1-config --input-config-file hp.yaml --input-training ${ML_TRAIN_RUN_LABEL} --input-inference ${ETL_INF_RUN_LABEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b50a3d-a9e5-43a9-aff9-dcd44fa351a4",
   "metadata": {},
   "source": [
    "## Running the pipeline\n",
    "These stages can be inspected with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141181e-6424-4c28-86c0-6c729fa25dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc dag --dot app_ml/ml_dataset_v1/model_name_v2/inference/${ML_INF_RUN_LABEL}/dvc.yaml | tee app_ml/ml_dataset_v1/model_name_v2/inference/${ML_INF_RUN_LABEL}/dvc_dag.dot\n",
    "if [[ $(command -v dot) ]]; then\n",
    "    dot -Tsvg app_ml/ml_dataset_v1/model_name_v2/inference/${ML_INF_RUN_LABEL}/dvc_dag.dot > app_ml/ml_dataset_v1/model_name_v2/inference/${ML_INF_RUN_LABEL}/dvc_dag.svg\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cc186-b78b-4df0-a900-713b4e4168b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(filename='app_ml/ml_dataset_v1/model_name_v2/inference/' + os.environ['ML_INF_RUN_LABEL'] + '/dvc_dag.svg'))  # test_ml_tutorial: skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093f914-a50b-4ee0-b94e-579cf8ec5c82",
   "metadata": {},
   "source": [
    "And finally executed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec8894-90c8-432a-876c-443a77333032",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dvc repro app_ml/ml_dataset_v1/model_name_v2/inference/${ML_INF_RUN_LABEL}/dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18e7364-2734-4b47-a0d9-19d080938585",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree in app_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056535e7-bb2b-41ec-aff8-b9c647f5c568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
